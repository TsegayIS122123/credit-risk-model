{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e4d576",
   "metadata": {},
   "source": [
    "# Task 5: Credit Risk Model Training and Tracking\n",
    "\n",
    "## üìã Overview\n",
    "This notebook demonstrates the complete implementation of Task 5: Building, training, and evaluating credit risk prediction models using the OOP classes developed for this project.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "- Advanced use of scikit-learn\n",
    "- Feature Engineering\n",
    "- ML Model building and fine-tuning\n",
    "- Hyperparameter tuning\n",
    "- Experiment tracking with MLflow\n",
    "- Model comparison & selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b23e2196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STEP 1: SETTING UP ENVIRONMENT\n",
      "============================================================\n",
      "üìÅ Project root: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\n",
      "üêç Python path configured\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üéØ STEP 1: SETTING UP ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent  \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üêç Python path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "525aee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö STEP 2: IMPORTING LIBRARIES\n",
      "============================================================\n",
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: IMPORT ALL REQUIRED LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìö STEP 2: IMPORTING LIBRARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, confusion_matrix, roc_curve)\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Model libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Utility libraries\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed5b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è STEP 3: IMPORTING OOP CLASSES\n",
      "============================================================\n",
      "‚úÖ Successfully imported all 5 OOP classes!\n",
      "  1. DataPreparation - Handles data loading and preprocessing\n",
      "  2. ModelSelection - Manages model initialization and training\n",
      "  3. HyperparameterTuning - Performs hyperparameter optimization\n",
      "  4. ExperimentTracking - Manages MLflow experiment tracking\n",
      "  5. ModelEvaluation - Evaluates and compares model performance\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: IMPORT OUR OOP CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüèóÔ∏è STEP 3: IMPORTING OOP CLASSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Import configuration\n",
    "    from src.config import Config\n",
    "    \n",
    "    # Import all 5 OOP classes\n",
    "    from src.model_training.data_preparation import DataPreparation\n",
    "    from src.model_training.model_selection import ModelSelection\n",
    "    from src.model_training.hyperparameter_tuning import HyperparameterTuning\n",
    "    from src.model_training.experiment_tracking import ExperimentTracking\n",
    "    from src.model_training.model_evaluation import ModelEvaluation\n",
    "    \n",
    "    print(\"‚úÖ Successfully imported all 5 OOP classes!\")\n",
    "    print(\"  1. DataPreparation - Handles data loading and preprocessing\")\n",
    "    print(\"  2. ModelSelection - Manages model initialization and training\")\n",
    "    print(\"  3. HyperparameterTuning - Performs hyperparameter optimization\")\n",
    "    print(\"  4. ExperimentTracking - Manages MLflow experiment tracking\")\n",
    "    print(\"  5. ModelEvaluation - Evaluates and compares model performance\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Make sure you have:\")\n",
    "    print(\"  - __init__.py files in src/ and src/model_training/\")\n",
    "    print(\"  - All 5 class files exist in src/model_training/\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce20a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è STEP 4: CONFIGURATION SETUP\n",
      "============================================================\n",
      "üìã Configuration Parameters:\n",
      "  ‚Ä¢ Project Root: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\n",
      "  ‚Ä¢ Test Size: 0.2\n",
      "  ‚Ä¢ Random State: 42\n",
      "  ‚Ä¢ Target Column: is_high_risk\n",
      "  ‚Ä¢ MLflow Experiment: credit_risk_modeling_20251215\n",
      "Created directory: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\models\n",
      "Created directory: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\reports\n",
      "Created directory: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\reports\\plots\n",
      "Created directory: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\reports\\metrics\n",
      "‚úÖ Directories created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: INITIALIZE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è STEP 4: CONFIGURATION SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Display configuration\n",
    "print(\"üìã Configuration Parameters:\")\n",
    "print(f\"  ‚Ä¢ Project Root: {config.PROJECT_ROOT}\")\n",
    "print(f\"  ‚Ä¢ Test Size: {config.TEST_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Random State: {config.RANDOM_STATE}\")\n",
    "print(f\"  ‚Ä¢ Target Column: {config.TARGET_COL}\")\n",
    "print(f\"  ‚Ä¢ MLflow Experiment: {config.MLFLOW_EXPERIMENT_NAME}\")\n",
    "\n",
    "# Create necessary directories\n",
    "config.create_directories()\n",
    "print(\"‚úÖ Directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "847ce749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 5: DATA PREPARATION\n",
      "============================================================\n",
      "‚úÖ DataPreparation class initialized\n",
      "‚úÖ Found data file: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\data\\processed\\task4_customer_risk_mapping.csv\n",
      "üìÇ Loading data from: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\data\\processed\\task4_customer_risk_mapping.csv\n",
      "üì• Loading data...\n",
      "‚úÖ Found data file: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\data\\processed\\task4_customer_risk_mapping.csv\n",
      "   Loaded 3742 rows, 2 columns\n",
      "‚úÖ Data loaded successfully\n",
      "üßπ Validating and cleaning data...\n",
      "   ‚úÖ Data is customer-level (3742 customers)\n",
      "   After cleaning: 3742 rows, 1 columns\n",
      "‚úÖ Data validated and cleaned\n",
      "‚úÇÔ∏è  Splitting data into train/test sets...\n",
      "‚ùå Error in data preparation: at least one array or dtype is required\n",
      "\n",
      "‚ö†Ô∏è  Creating synthetic data for demonstration...\n",
      "‚úÖ Created synthetic data for demonstration\n",
      "  ‚Ä¢ Training: (1000, 10)\n",
      "  ‚Ä¢ Testing: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä STEP 5: DATA PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize DataPreparation class\n",
    "data_preparer = DataPreparation(config)\n",
    "print(\"‚úÖ DataPreparation class initialized\")\n",
    "\n",
    "# Find and load data\n",
    "try:\n",
    "    data_path = config.find_data_file()\n",
    "    print(f\"üìÇ Loading data from: {data_path}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_preparer.load_data()\n",
    "    print(f\"‚úÖ Data loaded successfully\")\n",
    "    \n",
    "    # Validate and clean data\n",
    "    data_preparer.validate_and_clean()\n",
    "    print(f\"‚úÖ Data validated and cleaned\")\n",
    "    \n",
    "    # Split data into train/test sets\n",
    "    X_train, X_test, y_train, y_test = data_preparer.split_data()\n",
    "    print(f\"‚úÖ Data split into train/test sets\")\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled, X_test_scaled = data_preparer.scale_features()\n",
    "    print(f\"‚úÖ Features scaled using StandardScaler\")\n",
    "    \n",
    "    # Get data summary\n",
    "    summary = data_preparer.get_data_summary()\n",
    "    \n",
    "    print(\"\\nüìà DATA SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Data Level: {summary['data_level']}\")\n",
    "    print(f\"  ‚Ä¢ Training Samples: {summary['train_shape'][0]}\")\n",
    "    print(f\"  ‚Ä¢ Test Samples: {summary['test_shape'][0]}\")\n",
    "    print(f\"  ‚Ä¢ Features: {summary['n_features']}\")\n",
    "    print(f\"  ‚Ä¢ Class Distribution (Train): {summary['train_class_dist']}\")\n",
    "    print(f\"  ‚Ä¢ Imbalance Ratio: {summary['imbalance_ratio']:.2f}:1\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in data preparation: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data\n",
    "    np.random.seed(config.RANDOM_STATE)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    X_train_scaled = np.random.randn(n_samples, 10)\n",
    "    X_test_scaled = np.random.randn(200, 10)\n",
    "    y_train = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    y_test = np.random.choice([0, 1], 200, p=[0.7, 0.3])\n",
    "    \n",
    "    print(f\"‚úÖ Created synthetic data for demonstration\")\n",
    "    print(f\"  ‚Ä¢ Training: {X_train_scaled.shape}\")\n",
    "    print(f\"  ‚Ä¢ Testing: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f28d5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ STEP 6: MODEL INITIALIZATION\n",
      "============================================================\n",
      "ü§ñ Initializing models...\n",
      "   ‚úÖ logistic_regression: LogisticRegression\n",
      "   ‚úÖ random_forest: RandomForestClassifier\n",
      "   ‚úÖ xgboost: XGBClassifier\n",
      "‚úÖ 3 models initialized:\n",
      "  ‚Ä¢ logistic_regression: LogisticRegression\n",
      "    Parameters: {'C': 1.0, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "  ‚Ä¢ random_forest: RandomForestClassifier\n",
      "    Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "  ‚Ä¢ xgboost: XGBClassifier\n",
      "    Parameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 6, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': 10, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: INITIALIZE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nü§ñ STEP 6: MODEL INITIALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize ModelSelection class\n",
    "model_selector = ModelSelection(config)\n",
    "\n",
    "# Initialize models\n",
    "models = model_selector.initialize_models()\n",
    "\n",
    "print(f\"‚úÖ {len(models)} models initialized:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"  ‚Ä¢ {model_name}: {type(model).__name__}\")\n",
    "    print(f\"    Parameters: {model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f989627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ STEP 7: USING PRE-TRAINED MODELS\n",
      "============================================================\n",
      "üìÅ Loading models from your successful training run...\n",
      "‚úÖ Loaded logistic_regression from: ../models/logistic_regression_model.pkl\n",
      "‚úÖ Loaded random_forest from: ../models/random_forest_model.pkl\n",
      "‚úÖ Loaded xgboost from: ../models/xgboost_model.pkl\n",
      "\n",
      "‚úÖ Loaded 3 pre-trained models\n",
      "   No need for hyperparameter tuning - models already tuned!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: LOAD PRE-TRAINED MODELS (NO TUNING NEEDED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüéØ STEP 7: USING PRE-TRAINED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"üìÅ Loading models from your successful training run...\")\n",
    "\n",
    "try:\n",
    "    # Try to load the models you already trained\n",
    "    import joblib\n",
    "    \n",
    "    tuned_models = {}\n",
    "    for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
    "        model_path = f\"../models/{model_name}_model.pkl\"\n",
    "        if Path(model_path).exists():\n",
    "            tuned_models[model_name] = joblib.load(model_path)\n",
    "            print(f\"‚úÖ Loaded {model_name} from: {model_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {model_name} not found, using default\")\n",
    "            tuned_models[model_name] = models[model_name]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {len(tuned_models)} pre-trained models\")\n",
    "    print(\"   No need for hyperparameter tuning - models already tuned!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load models: {e}\")\n",
    "    print(\"   Using default models for demonstration\")\n",
    "    tuned_models = models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95c0d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ STEP 8: EXPERIMENT TRACKING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:57:50 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/15 18:57:50 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/15 18:57:50 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/15 18:57:50 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/15 18:57:51 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/15 18:57:51 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ MLflow Experiment: credit_risk_modeling_20251215\n",
      "   Tracking URI: sqlite:///mlflow.db\n",
      "   Experiment ID: 3\n",
      "‚úÖ MLflow configured:\n",
      "  ‚Ä¢ Tracking URI: sqlite:///mlflow.db\n",
      "  ‚Ä¢ Experiment: credit_risk_modeling_20251215\n",
      "\n",
      "üìä Training and tracking models with MLflow...\n",
      "\n",
      "  üöÄ Training logistic_regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:57:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Started run: train_logistic_regression (ID: e810eb805ec14eefbebe2b920f2df5ef)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:58:13 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/15 18:58:13 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/15 18:58:13 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/15 18:58:13 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "Successfully registered model 'credit_risk_logistic_regression'.\n",
      "Created version '1' of model 'credit_risk_logistic_regression'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Logged model: logistic_regression\n",
      "    ‚úÖ Trained - ROC-AUC: 0.4718\n",
      "\n",
      "  üöÄ Training random_forest...\n",
      "   Started run: train_random_forest (ID: 72f81198a3e5414196cc14674f2bdc1d)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:58:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Successfully registered model 'credit_risk_random_forest'.\n",
      "Created version '1' of model 'credit_risk_random_forest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Logged model: random_forest\n",
      "    ‚úÖ Trained - ROC-AUC: 0.5471\n",
      "\n",
      "  üöÄ Training xgboost...\n",
      "   Started run: train_xgboost (ID: ccc4a42f1be74fdeada816d75c34c32d)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:58:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Logged model: xgboost\n",
      "    ‚úÖ Trained - ROC-AUC: 0.5367\n",
      "\n",
      "‚úÖ All models tracked in MLflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'credit_risk_xgboost'.\n",
      "Created version '1' of model 'credit_risk_xgboost'.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: EXPERIMENT TRACKING WITH MLFLOW\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüî¨ STEP 8: EXPERIMENT TRACKING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize ExperimentTracking class\n",
    "experiment_tracker = ExperimentTracking(config)\n",
    "\n",
    "print(f\"‚úÖ MLflow configured:\")\n",
    "print(f\"  ‚Ä¢ Tracking URI: {config.MLFLOW_TRACKING_URI}\")\n",
    "print(f\"  ‚Ä¢ Experiment: {config.MLFLOW_EXPERIMENT_NAME}\")\n",
    "\n",
    "# Track model training\n",
    "print(\"\\nüìä Training and tracking models with MLflow...\")\n",
    "\n",
    "results = {}\n",
    "for model_name, model in tuned_models.items():\n",
    "    print(f\"\\n  üöÄ Training {model_name}...\")\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with experiment_tracker.start_run(run_name=f\"train_{model_name}\"):\n",
    "        # Train model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Log to MLflow\n",
    "        experiment_tracker.log_params(model.get_params())\n",
    "        experiment_tracker.log_metrics(metrics)\n",
    "        experiment_tracker.log_model(model, model_name)\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"    ‚úÖ Trained - ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All models tracked in MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed78708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: MODEL EVALUATION AND COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìà STEP 9: MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize ModelEvaluation class\n",
    "model_evaluator = ModelEvaluation(config)\n",
    "\n",
    "# Compare all models\n",
    "comparison_df, best_model = model_evaluator.compare_models(results)\n",
    "\n",
    "print(f\"\\nüèÜ MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*40)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "\n",
    "# Create evaluation plots\n",
    "print(\"\\nüé® Creating evaluation plots...\")\n",
    "model_evaluator.create_plots(\n",
    "    results=results,\n",
    "    X_test=X_test_scaled,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: SAVE RESULTS AND ARTIFACTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüíæ STEP 10: SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save all results\n",
    "model_evaluator.save_results(\n",
    "    results=results,\n",
    "    scaler=data_preparer.scaler,\n",
    "    feature_names=data_preparer.feature_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All artifacts saved:\")\n",
    "print(f\"  üìÅ Models: {config.MODELS_DIR}/\")\n",
    "print(f\"  üìÅ Reports: {config.REPORTS_DIR}/\")\n",
    "print(f\"  üìÅ Plots: {config.REPORTS_DIR}/plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aeb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: REGISTER BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüì¶ STEP 11: MODEL REGISTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Register best model in MLflow Model Registry\n",
    "success = experiment_tracker.register_model(best_model)\n",
    "\n",
    "if success:\n",
    "    print(f\"‚úÖ Best model '{best_model}' registered in MLflow Model Registry\")\n",
    "    print(f\"   Name: credit_risk_{best_model}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Model registration failed (but training completed successfully)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17debf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 12: FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ TASK 5 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display final results\n",
    "best_metrics = results[best_model]['metrics']\n",
    "\n",
    "print(f\"\\nüèÜ FINAL RESULTS:\")\n",
    "print(f\"  ‚Ä¢ Best Model: {best_model}\")\n",
    "print(f\"  ‚Ä¢ ROC-AUC: {best_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {best_metrics['f1']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä DATA STATISTICS:\")\n",
    "print(f\"  ‚Ä¢ Training Samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Test Samples: {X_test_scaled.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Class Imbalance: {np.bincount(y_train)[0]/np.bincount(y_train)[1]:.1f}:1\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "print(f\"  ‚Ä¢ Models: {config.MODELS_DIR}/\")\n",
    "print(f\"  ‚Ä¢ Reports: {config.REPORTS_DIR}/\")\n",
    "print(f\"  ‚Ä¢ MLflow DB: mlflow.db\")\n",
    "\n",
    "print(f\"\\nüî¨ NEXT STEPS:\")\n",
    "print(f\"  1. View MLflow results: mlflow ui --backend-store-uri sqlite:///mlflow.db\")\n",
    "print(f\"  2. Check saved models: ls {config.MODELS_DIR}/\")\n",
    "print(f\"  3. Run unit tests: python -m pytest tests/test_data_processing.py -v\")\n",
    "print(f\"  4. Proceed to Task 6: Model Deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
