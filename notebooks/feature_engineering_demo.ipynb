{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0948e6c",
   "metadata": {},
   "source": [
    "# Task 3: Feature Engineering - Step-by-Step Implementation\n",
    "\n",
    "## üìã Objective\n",
    "Build a robust, automated, and reproducible data processing script that transforms raw data into a model-ready format using sklearn.pipeline.Pipeline.\n",
    "\n",
    "## üéØ The 6 Required Steps:\n",
    "1. **Create Aggregate Features**\n",
    "2. **Extract Temporal Features**  \n",
    "3. **Encode Categorical Variables**\n",
    "4. **Handle Missing Values**\n",
    "5. **Normalize/Standardize Numerical Features**\n",
    "6. **Feature Engineering with WoE and IV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3924c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries and classes imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')  # Add src to path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our OOP classes\n",
    "from src.feature_engineering import (\n",
    "    AggregateFeatures,\n",
    "    TemporalFeatureExtractor,\n",
    "    CategoricalEncoder,\n",
    "    MissingValueHandler,\n",
    "    FeatureScaler,\n",
    "    WOETransformer,\n",
    "    FeatureEngineeringPipeline\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Libraries and classes imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc48a97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ LOADING DATA\n",
      "============================================================\n",
      "Data path: c:\\Users\\HP\\Desktop\\KAIM\\credit-risk-model\\data\\raw\\data.csv\n",
      "‚úÖ Loaded: 95,662 rows √ó 16 columns\n",
      "Columns: ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'Amount', 'Value', 'TransactionStartTime', 'PricingStrategy', 'FraudResult']\n",
      "\n",
      "üìÑ Sample data (first 3 rows):\n",
      "         TransactionId        BatchId       AccountId       SubscriptionId  \\\n",
      "0  TransactionId_76871  BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
      "1  TransactionId_73770  BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
      "2  TransactionId_26203  BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
      "\n",
      "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
      "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
      "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
      "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
      "\n",
      "      ProductCategory    ChannelId  Amount  Value  TransactionStartTime  \\\n",
      "0             airtime  ChannelId_3  1000.0   1000  2018-11-15T02:18:49Z   \n",
      "1  financial_services  ChannelId_2   -20.0     20  2018-11-15T02:19:08Z   \n",
      "2             airtime  ChannelId_3   500.0    500  2018-11-15T02:44:21Z   \n",
      "\n",
      "   PricingStrategy  FraudResult  \n",
      "0                2            0  \n",
      "1                2            0  \n",
      "2                2            0  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"üìÇ LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Determine correct path\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    data_path = current_dir.parent / 'data' / 'raw' / 'data.csv'\n",
    "else:\n",
    "    data_path = current_dir / 'data' / 'raw' / 'data.csv'\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"‚úÖ Loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìÑ Sample data (first 3 rows):\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e2a7f",
   "metadata": {},
   "source": [
    "## üéØ Step 1: Create Aggregate Features\n",
    "\n",
    "### What we're doing:\n",
    "Creating customer-level summary statistics from transaction data.\n",
    "\n",
    "### Required features:\n",
    "- **Total Transaction Amount**: Sum of all transaction amounts per customer\n",
    "- **Average Transaction Amount**: Average transaction amount per customer  \n",
    "- **Transaction Count**: Number of transactions per customer\n",
    "- **Standard Deviation**: Variability of transaction amounts per customer\n",
    "\n",
    "### OOP Class: `AggregateFeatures`\n",
    "This class groups by CustomerId and calculates all required statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec219bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ STEP 1: CREATING AGGREGATE FEATURES\n",
      "============================================================\n",
      "‚úÖ Step 1: Added 7 aggregate features\n",
      "\n",
      "üìä New aggregate features created:\n",
      "  ‚Ä¢ TotalAmount\n",
      "  ‚Ä¢ AvgAmount\n",
      "  ‚Ä¢ TransactionCount\n",
      "  ‚Ä¢ StdAmount\n",
      "  ‚Ä¢ MinAmount\n",
      "  ‚Ä¢ MaxAmount\n",
      "  ‚Ä¢ MedianAmount\n",
      "\n",
      "üßë‚Äçüíº Sample Customer: CustomerId_4406\n",
      "   Total Amount: $109,921.75\n",
      "   Average Amount: $923.71\n",
      "   Transaction Count: 119\n",
      "   Amount Std Dev: $3,042.29\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1: CREATE AGGREGATE FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"üî¢ STEP 1: CREATING AGGREGATE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize the transformer\n",
    "aggregator = AggregateFeatures(customer_col='CustomerId', amount_col='Amount')\n",
    "\n",
    "# Fit and transform\n",
    "df_step1 = aggregator.fit_transform(df)\n",
    "\n",
    "# Check what was added\n",
    "new_cols = [col for col in df_step1.columns if col not in df.columns]\n",
    "print(f\"\\nüìä New aggregate features created:\")\n",
    "for col in new_cols:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "# Display statistics for a sample customer\n",
    "sample_customer = df_step1['CustomerId'].iloc[0]\n",
    "customer_data = df_step1[df_step1['CustomerId'] == sample_customer].iloc[0]\n",
    "\n",
    "print(f\"\\nüßë‚Äçüíº Sample Customer: {sample_customer}\")\n",
    "print(f\"   Total Amount: ${customer_data['TotalAmount']:,.2f}\")\n",
    "print(f\"   Average Amount: ${customer_data['AvgAmount']:,.2f}\")\n",
    "print(f\"   Transaction Count: {customer_data['TransactionCount']}\")\n",
    "print(f\"   Amount Std Dev: ${customer_data['StdAmount']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc593d38",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Extract Temporal Features\n",
    "\n",
    "### What we're doing:\n",
    "Extracting time-based features from the TransactionStartTime column.\n",
    "\n",
    "### Required features:\n",
    "- **Transaction Hour**: Hour of day (0-23)\n",
    "- **Transaction Day**: Day of month (1-31)\n",
    "- **Transaction Month**: Month (1-12)\n",
    "- **Transaction Year**: Year\n",
    "\n",
    "### OOP Class: `TemporalFeatureExtractor`\n",
    "This class parses datetime and extracts all time components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cea89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ STEP 2: EXTRACTING TEMPORAL FEATURES\n",
      "============================================================\n",
      "‚úÖ Step 2: Added 7 temporal features\n",
      "\n",
      "üìÖ Temporal features created:\n",
      "  ‚Ä¢ TransactionHour\n",
      "  ‚Ä¢ TransactionDay\n",
      "  ‚Ä¢ TransactionMonth\n",
      "  ‚Ä¢ TransactionYear\n",
      "  ‚Ä¢ TransactionDayOfWeek\n",
      "  ‚Ä¢ TransactionWeekOfYear\n",
      "  ‚Ä¢ IsWeekend\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2: EXTRACT TEMPORAL FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n‚è∞ STEP 2: EXTRACTING TEMPORAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize the transformer\n",
    "temporal_extractor = TemporalFeatureExtractor(datetime_col='TransactionStartTime')\n",
    "\n",
    "# Fit and transform\n",
    "df_step2 = temporal_extractor.fit_transform(df_step1)\n",
    "\n",
    "# Check what was added\n",
    "temporal_cols = ['TransactionHour', 'TransactionDay', 'TransactionMonth', \n",
    "                 'TransactionYear', 'TransactionDayOfWeek', 'TransactionWeekOfYear', 'IsWeekend']\n",
    "\n",
    "print(f\"\\nüìÖ Temporal features created:\")\n",
    "for col in temporal_cols:\n",
    "    if col in df_step2.columns:\n",
    "        print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "# Visualize transaction patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Hourly distribution\n",
    "hour_counts = df_step2['TransactionHour'].value_counts().sort_index()\n",
    "axes[0, 0].bar(hour_counts.index, hour_counts.values)\n",
    "axes[0, 0].set_title('Transactions by Hour')\n",
    "axes[0, 0].set_xlabel('Hour')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Day of week distribution\n",
    "dow_counts = df_step2['TransactionDayOfWeek'].value_counts().sort_index()\n",
    "axes[0, 1].bar(dow_counts.index, dow_counts.values)\n",
    "axes[0, 1].set_title('Transactions by Day of Week')\n",
    "axes[0, 1].set_xlabel('Day (0=Monday)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Monthly distribution\n",
    "month_counts = df_step2['TransactionMonth'].value_counts().sort_index()\n",
    "axes[1, 0].bar(month_counts.index, month_counts.values)\n",
    "axes[1, 0].set_title('Transactions by Month')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Weekend vs weekday\n",
    "weekend_counts = df_step2['IsWeekend'].value_counts()\n",
    "axes[1, 1].pie(weekend_counts.values, labels=['Weekday', 'Weekend'], autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Weekend vs Weekday Transactions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff6515",
   "metadata": {},
   "source": [
    "## üéØ Step 3: Encode Categorical Variables\n",
    "\n",
    "### What we're doing:\n",
    "Converting categorical text data into numerical format for modeling.\n",
    "\n",
    "### Required methods:\n",
    "- **One-Hot Encoding**: Creates binary columns for each category\n",
    "- **Label Encoding**: Assigns integer IDs to each category\n",
    "\n",
    "### OOP Class: `CategoricalEncoder`\n",
    "This class automatically detects categorical columns and applies encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1433a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ STEP 3: ENCODING CATEGORICAL VARIABLES\n",
      "============================================================\n",
      "Found 10 categorical columns:\n",
      "  ‚Ä¢ TransactionId: 95662 unique values\n",
      "  ‚Ä¢ BatchId: 94809 unique values\n",
      "  ‚Ä¢ AccountId: 3633 unique values\n",
      "  ‚Ä¢ SubscriptionId: 3627 unique values\n",
      "  ‚Ä¢ CustomerId: 3742 unique values\n",
      "  ‚Ä¢ CurrencyCode: 1 unique values\n",
      "  ‚Ä¢ ProviderId: 6 unique values\n",
      "  ‚Ä¢ ProductId: 23 unique values\n",
      "  ‚Ä¢ ProductCategory: 9 unique values\n",
      "  ‚Ä¢ ChannelId: 4 unique values\n",
      "‚úÖ Step 3: Encoded 2 categorical columns using onehot encoding\n",
      "\n",
      "üéØ Created 13 encoded columns:\n",
      "  ‚Ä¢ ProductCategory_airtime\n",
      "  ‚Ä¢ ProductCategory_data_bundles\n",
      "  ‚Ä¢ ProductCategory_financial_services\n",
      "  ‚Ä¢ ProductCategory_movies\n",
      "  ‚Ä¢ ProductCategory_other\n",
      "  ‚Ä¢ ProductCategory_ticket\n",
      "  ‚Ä¢ ProductCategory_transport\n",
      "  ‚Ä¢ ProductCategory_tv\n",
      "  ‚Ä¢ ProductCategory_utility_bill\n",
      "  ‚Ä¢ ChannelId_ChannelId_1\n",
      "  ... and 3 more\n",
      "\n",
      "üìä Before Encoding (ProductCategory):\n",
      "ProductCategory\n",
      "financial_services    45405\n",
      "airtime               45027\n",
      "utility_bill           1920\n",
      "data_bundles           1613\n",
      "tv                     1279\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä After One-Hot Encoding:\n",
      "ProductCategory_airtime: 45,027.0 transactions\n",
      "ProductCategory_data_bundles: 1,613.0 transactions\n",
      "ProductCategory_financial_services: 45,405.0 transactions\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3: ENCODE CATEGORICAL VARIABLES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüî§ STEP 3: ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_step2.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Found {len(categorical_cols)} categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = df_step2[col].nunique()\n",
    "    print(f\"  ‚Ä¢ {col}: {unique_count} unique values\")\n",
    "\n",
    "# Initialize encoder (using one-hot for demonstration)\n",
    "encoder = CategoricalEncoder(strategy='onehot', columns=['ProductCategory', 'ChannelId'])\n",
    "\n",
    "# Fit and transform\n",
    "df_step3 = encoder.fit_transform(df_step2)\n",
    "\n",
    "# Show encoding results\n",
    "encoded_cols = [col for col in df_step3.columns if 'ProductCategory_' in col or 'ChannelId_' in col]\n",
    "print(f\"\\nüéØ Created {len(encoded_cols)} encoded columns:\")\n",
    "for col in encoded_cols[:10]:  # Show first 10\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "if len(encoded_cols) > 10:\n",
    "    print(f\"  ... and {len(encoded_cols) - 10} more\")\n",
    "\n",
    "# Show before/after comparison\n",
    "print(\"\\nüìä Before Encoding (ProductCategory):\")\n",
    "print(df_step2['ProductCategory'].value_counts().head())\n",
    "\n",
    "print(\"\\nüìä After One-Hot Encoding:\")\n",
    "for col in [c for c in encoded_cols if 'ProductCategory' in c][:3]:\n",
    "    print(f\"{col}: {df_step3[col].sum():,} transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309da448",
   "metadata": {},
   "source": [
    "## üéØ Step 4: Handle Missing Values\n",
    "\n",
    "### What we're doing:\n",
    "Identifying and treating missing data to ensure data quality.\n",
    "\n",
    "### Required methods:\n",
    "- **Imputation**: Fill missing values with mean, median, mode, or KNN\n",
    "- **Removal**: Remove rows/columns with too many missing values\n",
    "\n",
    "### OOP Class: `MissingValueHandler`\n",
    "This class analyzes missing patterns and applies appropriate treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ca2a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  STEP 4: HANDLING MISSING VALUES\n",
      "============================================================\n",
      "Found 1 columns with missing values:\n",
      "  ‚Ä¢ StdAmount: 712 missing (0.74%)\n",
      "‚úÖ Step 4: Handled missing values using median strategy\n",
      "\n",
      "‚úÖ Missing values after handling: 0\n",
      "\n",
      "üìà Data shape changes:\n",
      "  Before: 95,662 rows √ó 41 columns\n",
      "  After:  95,662 rows √ó 41 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 4: HANDLE MISSING VALUES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  STEP 4: HANDLING MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current missing values\n",
    "missing_before = df_step3.isnull().sum()\n",
    "missing_cols = missing_before[missing_before > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"Found {len(missing_cols)} columns with missing values:\")\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = (count / len(df_step3)) * 100\n",
    "        print(f\"  ‚Ä¢ {col}: {count:,} missing ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found in current data\")\n",
    "    \n",
    "    # Let's create some missing data for demonstration\n",
    "    print(\"\\nüß™ Creating sample missing data for demonstration...\")\n",
    "    df_demo = df_step3.copy()\n",
    "    df_demo.loc[df_demo.sample(frac=0.05).index, 'Amount'] = np.nan\n",
    "    df_demo.loc[df_demo.sample(frac=0.03).index, 'TransactionHour'] = np.nan\n",
    "    df_step3 = df_demo\n",
    "\n",
    "# Initialize missing value handler\n",
    "missing_handler = MissingValueHandler(strategy='median', remove_threshold=0.3)\n",
    "\n",
    "# Fit and transform\n",
    "df_step4 = missing_handler.fit_transform(df_step3)\n",
    "\n",
    "# Check results\n",
    "missing_after = df_step4.isnull().sum().sum()\n",
    "print(f\"\\n‚úÖ Missing values after handling: {missing_after:,}\")\n",
    "\n",
    "# Compare shapes\n",
    "print(f\"\\nüìà Data shape changes:\")\n",
    "print(f\"  Before: {df_step3.shape[0]:,} rows √ó {df_step3.shape[1]} columns\")\n",
    "print(f\"  After:  {df_step4.shape[0]:,} rows √ó {df_step4.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5716d56b",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Normalize/Standardize Numerical Features\n",
    "\n",
    "### What we're doing:\n",
    "Scaling numerical features to similar ranges for better model performance.\n",
    "\n",
    "### Required methods:\n",
    "- **Normalization**: Scale to [0, 1] range (MinMax)\n",
    "- **Standardization**: Scale to mean=0, std=1 (Z-score)\n",
    "\n",
    "### OOP Class: `FeatureScaler`\n",
    "This class detects numerical columns and applies chosen scaling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a223f891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìè STEP 5: NORMALIZING/STANDARDIZING NUMERICAL FEATURES\n",
      "============================================================\n",
      "Selected 17 numerical features for scaling:\n",
      "  ‚Ä¢ CountryCode\n",
      "  ‚Ä¢ Amount\n",
      "  ‚Ä¢ Value\n",
      "  ‚Ä¢ PricingStrategy\n",
      "  ‚Ä¢ TotalAmount\n",
      "  ‚Ä¢ AvgAmount\n",
      "  ‚Ä¢ TransactionCount\n",
      "  ‚Ä¢ StdAmount\n",
      "  ‚Ä¢ MinAmount\n",
      "  ‚Ä¢ MaxAmount\n",
      "  ... and 7 more\n",
      "‚úÖ Step 5: Scaled 17 numerical features using standard\n",
      "\n",
      "üìä Before/After Scaling Comparison:\n",
      "--------------------------------------------------\n",
      "\n",
      "Amount:\n",
      "  Before: Mean = 6717.85, Std = 123306.80\n",
      "  After:  Mean = -0.00, Std = 1.00\n",
      "\n",
      "Value:\n",
      "  Before: Mean = 9900.58, Std = 123122.09\n",
      "  After:  Mean = -0.00, Std = 1.00\n",
      "\n",
      "TransactionHour:\n",
      "  Before: Mean = 12.45, Std = 4.85\n",
      "  After:  Mean = -0.00, Std = 1.00\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 5: NORMALIZE/STANDARDIZE NUMERICAL FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüìè STEP 5: NORMALIZING/STANDARDIZING NUMERICAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select numerical columns to scale\n",
    "numerical_cols = df_step4.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Exclude binary/indicator columns\n",
    "exclude = ['IsWeekend', 'IsCredit', 'FraudResult'] + [col for col in numerical_cols if 'ProductCategory_' in col or 'ChannelId_' in col]\n",
    "scale_cols = [col for col in numerical_cols if col not in exclude]\n",
    "\n",
    "print(f\"Selected {len(scale_cols)} numerical features for scaling:\")\n",
    "for col in scale_cols[:10]:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "if len(scale_cols) > 10:\n",
    "    print(f\"  ... and {len(scale_cols) - 10} more\")\n",
    "\n",
    "# Initialize scaler (using standardization)\n",
    "scaler = FeatureScaler(strategy='standard', columns=scale_cols)\n",
    "\n",
    "# Fit and transform\n",
    "df_step5 = scaler.fit_transform(df_step4)\n",
    "\n",
    "# Show before/after comparison for sample features\n",
    "sample_features = ['Amount', 'Value', 'TransactionHour']\n",
    "print(\"\\nüìä Before/After Scaling Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for feature in sample_features:\n",
    "    if feature in df_step4.columns:\n",
    "        before_mean = df_step4[feature].mean()\n",
    "        before_std = df_step4[feature].std()\n",
    "        after_mean = df_step5[feature].mean()\n",
    "        after_std = df_step5[feature].std()\n",
    "        \n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Before: Mean = {before_mean:.2f}, Std = {before_std:.2f}\")\n",
    "        print(f\"  After:  Mean = {after_mean:.2f}, Std = {after_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e143d4",
   "metadata": {},
   "source": [
    "## üéØ Step 6: Feature Engineering with WoE and IV\n",
    "\n",
    "### What we're doing:\n",
    "Applying Weight of Evidence (WoE) transformation to create features with predictive power.\n",
    "\n",
    "### Key concepts:\n",
    "- **WoE (Weight of Evidence)**: Measures how much a feature category indicates \"good\" vs \"bad\"\n",
    "- **IV (Information Value)**: Measures overall predictive power of a feature\n",
    "\n",
    "### OOP Class: `WOETransformer`\n",
    "This class calculates WoE for each feature category and creates WoE-transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c7aaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ STEP 6: FEATURE ENGINEERING WITH WOE AND IV\n",
      "============================================================\n",
      "üìö Understanding WoE & IV:\n",
      "\n",
      "Weight of Evidence (WoE):\n",
      "‚Ä¢ Measures how much a feature category indicates 'good' vs 'bad'\n",
      "‚Ä¢ WoE = ln(% of Good / % of Bad)\n",
      "‚Ä¢ Positive WoE = More 'good' customers in this category\n",
      "‚Ä¢ Negative WoE = More 'bad' customers in this category\n",
      "\n",
      "Information Value (IV):\n",
      "‚Ä¢ Measures overall predictive power of a feature\n",
      "‚Ä¢ IV < 0.02: Not useful\n",
      "‚Ä¢ 0.02-0.1: Weak predictor\n",
      "‚Ä¢ 0.1-0.3: Medium predictor  \n",
      "‚Ä¢ 0.3-0.5: Strong predictor\n",
      "‚Ä¢ > 0.5: Suspicious (check for data leakage)\n",
      "\n",
      "\n",
      "üß™ Using FraudResult as target for WoE demonstration...\n",
      "Calculating WoE for 8 features...\n",
      "üîç Step 6: Calculating WoE/IV for 8 features...\n",
      "‚úÖ Step 6: Added 8 WoE features\n",
      "\n",
      "üìà INFORMATION VALUE (IV) REPORT:\n",
      "============================================================\n",
      "            Feature        IV Predictive_Power\n",
      "1             Value  4.279244       Suspicious\n",
      "0            Amount  4.173982       Suspicious\n",
      "6         AvgAmount  3.707218       Suspicious\n",
      "5       TotalAmount  3.359739       Suspicious\n",
      "7  TransactionCount  0.539702       Suspicious\n",
      "2   TransactionHour  0.100094           Medium\n",
      "4  TransactionMonth  0.095442             Weak\n",
      "3    TransactionDay  0.093363             Weak\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 6: FEATURE ENGINEERING WITH WOE AND IV\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüéØ STEP 6: FEATURE ENGINEERING WITH WOE AND IV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"üìö Understanding WoE & IV:\")\n",
    "print(\"\"\"\n",
    "Weight of Evidence (WoE):\n",
    "‚Ä¢ Measures how much a feature category indicates 'good' vs 'bad'\n",
    "‚Ä¢ WoE = ln(% of Good / % of Bad)\n",
    "‚Ä¢ Positive WoE = More 'good' customers in this category\n",
    "‚Ä¢ Negative WoE = More 'bad' customers in this category\n",
    "\n",
    "Information Value (IV):\n",
    "‚Ä¢ Measures overall predictive power of a feature\n",
    "‚Ä¢ IV < 0.02: Not useful\n",
    "‚Ä¢ 0.02-0.1: Weak predictor\n",
    "‚Ä¢ 0.1-0.3: Medium predictor  \n",
    "‚Ä¢ 0.3-0.5: Strong predictor\n",
    "‚Ä¢ > 0.5: Suspicious (check for data leakage)\n",
    "\"\"\")\n",
    "\n",
    "# For demonstration, use FraudResult as target\n",
    "print(\"\\nüß™ Using FraudResult as target for WoE demonstration...\")\n",
    "\n",
    "# Prepare data for WoE\n",
    "woe_features = ['Amount', 'Value', 'TransactionHour', 'TransactionDay', \n",
    "                'TransactionMonth', 'TotalAmount', 'AvgAmount', 'TransactionCount']\n",
    "\n",
    "X_woe = df_step5[woe_features].copy()\n",
    "y_woe = df_step5['FraudResult'].copy()  # Target variable\n",
    "\n",
    "print(f\"Calculating WoE for {len(woe_features)} features...\")\n",
    "\n",
    "# Initialize WoE transformer (NO target_col parameter needed)\n",
    "woe_transformer = WOETransformer(n_bins=5)\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Use fit() with X and y separately, NOT fit_transform()\n",
    "woe_transformer.fit(X_woe, y_woe)  # Pass y as second parameter\n",
    "\n",
    "# Then transform\n",
    "X_woe_transformed = woe_transformer.transform(X_woe)\n",
    "\n",
    "# Get IV report\n",
    "iv_report = woe_transformer.get_iv_report()\n",
    "print(\"\\nüìà INFORMATION VALUE (IV) REPORT:\")\n",
    "print(\"=\"*60)\n",
    "print(iv_report.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08ad31c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó MINIMAL PIPELINE FOR TASK 3 COMPLETION\n",
      "============================================================\n",
      "\n",
      "For Task 3 demonstration, we'll:\n",
      "1. DROP high-cardinality ID columns (not useful for modeling)\n",
      "2. Keep only meaningful features\n",
      "3. Complete all 6 steps without memory issues\n",
      "\n",
      "üìä Data after dropping IDs: (95662, 10)\n",
      "\n",
      "üöÄ Running minimal pipeline...\n",
      "‚úÖ Step 1: Added 7 aggregate features\n",
      "‚úÖ Step 2: Added 7 temporal features\n",
      "‚úÖ Step 3: Encoded 3 categorical columns using onehot encoding\n",
      "‚úÖ Step 4: Handled missing values using median strategy\n",
      "‚úÖ Step 5: Scaled 38 numerical features using standard\n",
      "\n",
      "‚úÖ MINIMAL PIPELINE RESULTS:\n",
      "============================================================\n",
      "Original: (95662, 16)\n",
      "After dropping IDs: (95662, 10)\n",
      "Final:    (95662, 40)\n",
      "\n",
      "üìä FINAL FEATURE TYPES:\n",
      "\n",
      "Temporal (8):\n",
      "  ‚Ä¢ TransactionStartTime\n",
      "  ‚Ä¢ TransactionCount\n",
      "  ‚Ä¢ TransactionHour\n",
      "  ‚Ä¢ TransactionDay\n",
      "  ‚Ä¢ TransactionMonth\n",
      "    ... and 3 more\n",
      "\n",
      "Encoded (13):\n",
      "  ‚Ä¢ ProductCategory_airtime\n",
      "  ‚Ä¢ ProductCategory_data_bundles\n",
      "  ‚Ä¢ ProductCategory_financial_services\n",
      "  ‚Ä¢ ProductCategory_movies\n",
      "  ‚Ä¢ ProductCategory_other\n",
      "    ... and 8 more\n",
      "\n",
      "Original (7):\n",
      "  ‚Ä¢ CustomerId\n",
      "  ‚Ä¢ CountryCode\n",
      "  ‚Ä¢ Amount\n",
      "  ‚Ä¢ Value\n",
      "  ‚Ä¢ TransactionStartTime\n",
      "    ... and 2 more\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MINIMAL PIPELINE (RECOMMENDED FOR TASK 3)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüîó MINIMAL PIPELINE FOR TASK 3 COMPLETION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "For Task 3 demonstration, we'll:\n",
    "1. DROP high-cardinality ID columns (not useful for modeling)\n",
    "2. Keep only meaningful features\n",
    "3. Complete all 6 steps without memory issues\n",
    "\"\"\")\n",
    "\n",
    "# Drop ID columns (not useful for credit risk modeling)\n",
    "id_columns = ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', \n",
    "              'ProductId', 'CurrencyCode']\n",
    "df_minimal = df.drop(columns=id_columns)\n",
    "\n",
    "print(f\"üìä Data after dropping IDs: {df_minimal.shape}\")\n",
    "\n",
    "# Create minimal pipeline\n",
    "minimal_pipeline = Pipeline([\n",
    "    ('step1_aggregate', AggregateFeatures()),\n",
    "    ('step2_temporal', TemporalFeatureExtractor()),\n",
    "    ('step3_encode', CategoricalEncoder(\n",
    "        strategy='onehot', \n",
    "        columns=['ProductCategory', 'ChannelId', 'ProviderId']\n",
    "    )),\n",
    "    ('step4_missing', MissingValueHandler(strategy='median')),\n",
    "    ('step5_scale', FeatureScaler(strategy='standard')),\n",
    "])\n",
    "\n",
    "print(\"\\nüöÄ Running minimal pipeline...\")\n",
    "df_final = minimal_pipeline.fit_transform(df_minimal)\n",
    "\n",
    "print(f\"\\n‚úÖ MINIMAL PIPELINE RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original: {df.shape}\")\n",
    "print(f\"After dropping IDs: {df_minimal.shape}\")\n",
    "print(f\"Final:    {df_final.shape}\")\n",
    "\n",
    "# Show what features we have\n",
    "print(\"\\nüìä FINAL FEATURE TYPES:\")\n",
    "feature_categories = {\n",
    "    'Temporal': [col for col in df_final.columns if 'Transaction' in col],\n",
    "    'Aggregate': [col for col in df_final.columns if 'Customer' in col and col != 'CustomerId'],\n",
    "    'Encoded': [col for col in df_final.columns if '_' in col and ('ProductCategory' in col or 'ChannelId' in col)],\n",
    "    'Original': [col for col in df_final.columns if col in df.columns]\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        print(f\"\\n{category} ({len(features)}):\")\n",
    "        for feat in features[:5]:\n",
    "            print(f\"  ‚Ä¢ {feat}\")\n",
    "        if len(features) > 5:\n",
    "            print(f\"    ... and {len(features) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65df6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ SAVING RESULTS\n",
      "============================================================\n",
      "‚úÖ Saved engineered features to:\n",
      "   üìÑ CSV: ..\\data\\processed\\task3_features_engineered.csv\n",
      "   üìä Parquet: ..\\data\\processed\\task3_features_engineered.parquet\n",
      "   üìè CSV size: 73.04 MB\n",
      "   üìè Parquet size: 2.12 MB\n",
      "\n",
      "üîç Verifying saved files:\n",
      "   CSV exists: True\n",
      "   Parquet exists: True\n",
      "\n",
      "üìÑ First 3 rows of saved data:\n",
      "        CustomerId  CountryCode    Amount     Value      TransactionStartTime  \\\n",
      "0  CustomerId_4406          0.0 -0.046371 -0.072291 2018-11-15 02:18:49+00:00   \n",
      "1  CustomerId_4406          0.0 -0.054643 -0.080251 2018-11-15 02:19:08+00:00   \n",
      "2  CustomerId_4683          0.0 -0.050426 -0.076352 2018-11-15 02:44:21+00:00   \n",
      "\n",
      "   PricingStrategy  FraudResult  TotalAmount  AvgAmount  TransactionCount  \\\n",
      "0        -0.349252    -0.044962     0.170118  -0.067623         -0.311831   \n",
      "1        -0.349252    -0.044962     0.170118  -0.067623         -0.311831   \n",
      "2        -0.349252    -0.044962     0.165122  -0.072568         -0.444993   \n",
      "\n",
      "   ...  ChannelId_ChannelId_1  ChannelId_ChannelId_2  ChannelId_ChannelId_3  \\\n",
      "0  ...              -0.075205              -0.796656               0.824740   \n",
      "1  ...              -0.075205               1.255247              -1.212503   \n",
      "2  ...              -0.075205              -0.796656               0.824740   \n",
      "\n",
      "   ChannelId_ChannelId_5  ProviderId_ProviderId_1  ProviderId_ProviderId_2  \\\n",
      "0              -0.105245                -0.250373                -0.013719   \n",
      "1              -0.105245                -0.250373                -0.013719   \n",
      "2              -0.105245                -0.250373                -0.013719   \n",
      "\n",
      "   ProviderId_ProviderId_3  ProviderId_ProviderId_4  ProviderId_ProviderId_5  \\\n",
      "0                -0.182517                -0.815149                -0.423397   \n",
      "1                -0.182517                 1.226769                -0.423397   \n",
      "2                -0.182517                -0.815149                -0.423397   \n",
      "\n",
      "   ProviderId_ProviderId_6  \n",
      "0                 1.341000  \n",
      "1                -0.745712  \n",
      "2                 1.341000  \n",
      "\n",
      "[3 rows x 40 columns]\n",
      "\n",
      "üìä Data types in saved file:\n",
      "float64                38\n",
      "object                  1\n",
      "datetime64[ns, UTC]     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SAVE RESULTS AND SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüíæ SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save processed data\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as CSV (human-readable) AND Parquet (efficient)\n",
    "csv_path = output_dir / 'task3_features_engineered.csv'\n",
    "parquet_path = output_dir / 'task3_features_engineered.parquet'\n",
    "\n",
    "# Save both formats\n",
    "df_final.to_csv(csv_path, index=False)\n",
    "df_final.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved engineered features to:\")\n",
    "print(f\"   üìÑ CSV: {csv_path}\")\n",
    "print(f\"   üìä Parquet: {parquet_path}\")\n",
    "print(f\"   üìè CSV size: {csv_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "print(f\"   üìè Parquet size: {parquet_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Verify the files exist\n",
    "print(f\"\\nüîç Verifying saved files:\")\n",
    "print(f\"   CSV exists: {csv_path.exists()}\")\n",
    "print(f\"   Parquet exists: {parquet_path.exists()}\")\n",
    "\n",
    "# Show sample of saved data\n",
    "print(f\"\\nüìÑ First 3 rows of saved data:\")\n",
    "print(df_final.head(3))\n",
    "\n",
    "# Show column types\n",
    "print(f\"\\nüìä Data types in saved file:\")\n",
    "print(df_final.dtypes.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
